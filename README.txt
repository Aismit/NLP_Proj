Language detection


Attached is a language dataset fabricated from Wikipedia datasets in different languages. The dataset is not manicured. I have included a training dataset, with both X and y labels, and a test dataset without y labels for the submission. I wanted to create a model that correctly predicts a language for a given text snippet.

    1) build_model script: should read in train_X_languages_homework.json.txt and train_y_languages_homework.json.txt to build, train, and tune my best predictive model. The script saves my best model to the filesystem and also log data about the expected performance of the model to a text file named performance.txt.

    2) make_predictions script: should load my saved model generated by the build_model script, read the file test_X_languages_homework.json.txt and output a predictions.txt file with my predictions, with one prediction per line in a similar format to train_y_languages_homework.json.txt.

    3) model binary: serialized version of my best model from running step 1.

    4) predictions.txt: one prediction per line in the same order as test_X_languages_homework.json.txt.

    5) performance.txt: the data about the expected performance of my model, and also how I evaluated the best model.

    6) notes.txt: includes time spent on the project, the software dependencies for my code, notes about how I chose my model, how I engineered my features, and what other options I wanted to explore but didn't have time currently.
